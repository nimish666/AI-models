{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-01T13:28:02.249345Z","iopub.execute_input":"2023-08-01T13:28:02.249784Z","iopub.status.idle":"2023-08-01T13:28:02.258895Z","shell.execute_reply.started":"2023-08-01T13:28:02.249746Z","shell.execute_reply":"2023-08-01T13:28:02.257865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\n","metadata":{"execution":{"iopub.status.busy":"2023-08-01T13:28:05.140494Z","iopub.execute_input":"2023-08-01T13:28:05.141751Z","iopub.status.idle":"2023-08-01T13:28:05.146809Z","shell.execute_reply.started":"2023-08-01T13:28:05.141689Z","shell.execute_reply":"2023-08-01T13:28:05.145569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nserving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ntrain_df.head(10)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-01T13:28:07.200772Z","iopub.execute_input":"2023-08-01T13:28:07.201130Z","iopub.status.idle":"2023-08-01T13:28:07.227594Z","shell.execute_reply.started":"2023-08-01T13:28:07.201102Z","shell.execute_reply":"2023-08-01T13:28:07.226546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    \n    def normalize_name(x):\n        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n    \n    def ticket_number(x):\n        return x.split(\" \")[-1]\n        \n    def ticket_item(x):\n        items = x.split(\" \")\n        if len(items) == 1:\n            return \"NONE\"\n        return \"_\".join(items[0:-1])\n    \n    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)                     \n    return df\n    \npreprocessed_train_df = preprocess(train_df)\npreprocessed_serving_df = preprocess(serving_df)\n\npreprocessed_train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T07:51:41.194819Z","iopub.execute_input":"2023-07-31T07:51:41.195261Z","iopub.status.idle":"2023-07-31T07:51:41.229140Z","shell.execute_reply.started":"2023-07-31T07:51:41.195227Z","shell.execute_reply":"2023-07-31T07:51:41.227976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_features = list(preprocessed_train_df.columns)\ninput_features.remove(\"Ticket\")\ninput_features.remove(\"PassengerId\")\ninput_features.remove(\"Survived\")\n#input_features.remove(\"Ticket_number\")\n\nprint(f\"Input features: {input_features}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-28T08:06:15.901681Z","iopub.execute_input":"2023-07-28T08:06:15.902073Z","iopub.status.idle":"2023-07-28T08:06:15.907875Z","shell.execute_reply.started":"2023-07-28T08:06:15.902045Z","shell.execute_reply":"2023-07-28T08:06:15.906557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_names(features, labels=None):\n    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n    return features, labels\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\nserving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T08:06:17.540591Z","iopub.execute_input":"2023-07-28T08:06:17.541277Z","iopub.status.idle":"2023-07-28T08:06:18.561134Z","shell.execute_reply.started":"2023-07-28T08:06:17.541247Z","shell.execute_reply":"2023-07-28T08:06:18.560297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    verbose=0, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    random_seed=1234,\n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-28T08:06:19.303442Z","iopub.execute_input":"2023-07-28T08:06:19.303799Z","iopub.status.idle":"2023-07-28T08:06:27.620411Z","shell.execute_reply.started":"2023-07-28T08:06:19.303776Z","shell.execute_reply":"2023-07-28T08:06:27.619557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    verbose=0, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    \n    #num_trees=2000,\n    \n    # Only for GBT.\n    # A bit slower, but great to understand the model.\n    # compute_permutation_variable_importance=True,\n    \n    # Change the default hyper-parameters\n    # hyperparameter_template=\"benchmark_rank1@v1\",\n    \n    #num_trees=1000,\n    #tuner=tuner\n    \n    min_examples=1,\n    categorical_algorithm=\"RANDOM\",\n    #max_depth=4,\n    shrinkage=0.05,\n    #num_candidate_attributes_ratio=0.2,\n    split_axis=\"SPARSE_OBLIQUE\",\n    sparse_oblique_normalization=\"MIN_MAX\",\n    sparse_oblique_num_projections_exponent=2.0,\n    num_trees=2000,\n    #validation_ratio=0.0,\n    random_seed=1234,\n    \n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T15:09:49.603974Z","iopub.execute_input":"2023-07-27T15:09:49.604751Z","iopub.status.idle":"2023-07-27T15:09:52.604255Z","shell.execute_reply.started":"2023-07-27T15:09:49.604690Z","shell.execute_reply":"2023-07-27T15:09:52.601613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nk=list(train_df.columns)\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error\nprint(k)\n#Import svm model\nfrom sklearn import svm\n\n\nenc = OneHotEncoder()\n\n#train_df.drop(['Survived'], axis=1)\ntrain_df['Embarked'] = train_df['Embarked'].fillna('C')\n\ntrain_df = train_df.fillna(0)\n#train_df.dropna() \n#train_df = train_df.reset_index(drop=True)\n#train_df.dropna(axis=0, how='all', subset=['Ticket', 'PassengerId','Name','Sex','Cabin','Embarked'], inplace=True)\n  \none_hot_encoded_data = pd.get_dummies(train_df, columns = ['Sex','Embarked'])\n\n\n#one_hot_encoded_data=one_hot_encoded_data.dropna() \n#one_hot_encoded_data= one_hot_encoded_data.reset_index(drop=True)\n#df.drop('a', inplace=True, axis=1)\n\n#train_x = train_df\n#label_encoder = preprocessing.LabelEncoder()\none_hot_encoded_data.drop(['Name'],inplace=True, axis=1)\none_hot_encoded_data.drop(['Ticket'],inplace=True, axis=1)\none_hot_encoded_data.drop(['Cabin'],inplace=True, axis=1)\n  \n# Encode labels in column 'species'.\n#train_df['Ticket', 'PassengerId','Name','Sex','Cabin','Embarked']= label_encoder.fit_transform(train_df['Ticket', 'PassengerId','Name','Sex','Cabin','Embarked'])\n\ny_train=one_hot_encoded_data['Survived']\ny_train = y_train.fillna(0)\n#y_train.dropna()\n#y_train = y_train.reset_index(drop=True)\n\n#X_train.columns = X_train.columns.astype(str)\none_hot_encoded_data.drop(['Survived'],inplace=True, axis=1)\n#train_df = train_df.fillna(0)\nX_train=one_hot_encoded_data\nprint('below is X_train')\n  \nprint(X_train)\n             \n\nclf = svm.SVC(kernel='linear') # Linear Kernel\n\n#Train the model using the training sets\nclf.fit(X_train, y_train)\n\n#Predict the response for test dataset\ndef predict(a):\n    \n    a = a.fillna(0)\n    one_hot_encoded_data = pd.get_dummies(a, columns = ['Sex','Embarked'])\n\n    one_hot_encoded_data=one_hot_encoded_data.dropna() \n    one_hot_encoded_data = one_hot_encoded_data.reset_index(drop=True)\n    one_hot_encoded_data.drop(['Name'],inplace=True, axis=1)\n    one_hot_encoded_data.drop(['Ticket'],inplace=True, axis=1)\n    one_hot_encoded_data.drop(['Cabin'],inplace=True, axis=1)\n    #y_train=one_hot_encoded_data['Survived']\n\n\n    #X_train.columns = X_train.columns.astype(str)\n    #one_hot_encoded_data.drop(['Survived'], axis=1)\n    X1_train=one_hot_encoded_data\n    y_pred = clf.predict(X1_train)\n    print(y_pred)\n    df1=a['PassengerId'].values\n    df1= pd.DataFrame(df1)\n    \n    y_pred= pd.DataFrame(y_pred)\n    submission=pd.concat([df1, y_pred], axis=1, keys=['PassengerId', 'Survived'])\n    #submission = df1.join(df2, how='outer', lsuffix='_df1', rsuffix='_df2')\n    print('below is sub file')\n    print(submission)\n    submission.to_csv('submission.csv')\n    \n    #mean_absolute_error(, y_pred)\npredict(serving_df)\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-01T13:31:02.183789Z","iopub.execute_input":"2023-08-01T13:31:02.184192Z","iopub.status.idle":"2023-08-01T13:31:25.782176Z","shell.execute_reply.started":"2023-08-01T13:31:02.184162Z","shell.execute_reply":"2023-08-01T13:31:25.781065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clf.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T11:59:43.076738Z","iopub.execute_input":"2023-08-01T11:59:43.078050Z","iopub.status.idle":"2023-08-01T11:59:43.109618Z","shell.execute_reply.started":"2023-08-01T11:59:43.078005Z","shell.execute_reply":"2023-08-01T11:59:43.108148Z"},"trusted":true},"execution_count":null,"outputs":[]}]}